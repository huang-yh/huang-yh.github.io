<!doctype html>
<html>
<head>
<title>SpectralAR</title>
<meta name="viewport" content="width=device-width,initial-scale=1">
<!-- <link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous"> -->
<link href="bootstrap.min.css" rel="stylesheet">
<!-- <script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script> -->
<!-- <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet"> -->
<link href="opensans.css" rel="stylesheet">
<link rel="icon" href="images/logo3.png">
<link href="style.css" rel="stylesheet">
<style>
  .container_2{
    position: relative;
    width: 100%;
    height: 0;
    padding-bottom: 56.25%;
}
.video {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
}

  .collapsible {
    background-color: #777;
    color: white;
    cursor: pointer;
    padding: 18px;
    width: 100%;
    border: none;
    text-align: left;
    outline: none;
    font-size: 15px;
  }

  .active, .collapsible:hover {
    background-color: #555;
  }
  
  .content {
    padding: 0 18px;
    max-height: 0;
    overflow: hidden;
    transition: max-height 0.2s ease-out;
    background-color: #f1f1f1;
  }
</style>

<style>
.paperthumb {
  float:left; width: 120px; margin: 3px 10px 7px 0;
}
.paperdesc {
  clear: both;
}
</style>
</head>

<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <p class="lead" style="font-size:30px">
    <b><a href="https://arxiv.org/abs/tobedone">SpectralAR: Spectral Autoregressive Visual Generation</a></b>
  <address style="font-size: 110%;">
    <nobr><a href="https://huang-yh.github.io/">Yuanhui Huang</a>,</nobr>
    <nobr><a href="chen-wl20.github.io">Weiliang Chen</a>,</nobr>
    <nobr><a href="https://wzzheng.net/">Wenzhao Zheng</a>,</nobr>
    <nobr><a href="https://scholar.google.com/citations?user=qDseo3cAAAAJ&hl=en">Yueqi Duan</a>,</nobr>
    <nobr><a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en">Jie Zhou</a>,</nobr>
    <nobr><a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
  <br>
      <nobr>Tsinghua University,</nobr>
      <nobr>University of California, Berkeley</nobr>
  </address>
   <!-- <div style="font-size: 170%;">CVPR 2023</div> -->
  <address style="font-size: 120%;">
	 <!-- <br> -->
  [<a href="https://arxiv.org/pdf/tobedone"><b>Paper (Arxiv)</b></a>]&nbsp;&nbsp;&nbsp;&nbsp;
  <!-- [<a href="https://www.youtube.com/">Video(Youtube)</a>]&nbsp;&nbsp;&nbsp;&nbsp; -->
  [<a href="https://github.com/tobedone"><b>Code (GitHub)</b></a>]&nbsp;&nbsp;&nbsp;&nbsp;
  <!-- [<a href="https://zhuanlan.zhihu.com">Post(Zhihu)</a>] -->
  </address>
  <!-- <small>*Equal contribution. † Project Leader. ‡Corresponding author.</small> -->
 </div>
 </p>
 </div>
</div> <!-- end nd-pageheader -->

<div class="container">


<p align="center">
  <video width="90%" controls>
    <source src="videos/demo.mov" type="video/mp4">
  </video>
</p>

<p align="center">
    <img src="images/teaser.png" width="90%">
</p>
<p><b>Overview of our contributions.</b>
Autoregressive visual generation has garnered increasing attention due to its scalability and compatibility with other modalities compared with diffusion models.
Most existing methods construct visual sequences as spatial patches for autoregressive generation.
However, image patches are inherently parallel, contradicting the causal nature of autoregressive modeling.
To address this, we propose a Spectral AutoRegressive (SpectralAR) visual generation framework, which realizes causality for visual sequences from the spectral perspective.
Specifically, we first transform an image into ordered spectral tokens with Nested Spectral Tokenization, representing lower to higher frequency components.
We then perform autoregressive generation in a coarse-to-fine manner with the sequences of spectral tokens.
By considering different levels of detail in images, our SpectralAR achieves both sequence causality and token efficiency without bells and whistles.
We conduct extensive experiments on ImageNet-1K for image reconstruction and autoregressive generation, and SpectralAR achieves 3.02 gFID with only 64 tokens and 310M parameters.
</p>


<h2>Nested Spectral Tokenization</h2><hr>
<p>
Different from the naive 1D tokenization, we aim to represent an image as its spectral decompositions for sequence causality.
We propose a nested mapping scheme for efficient tokenization by constructing a sequence of sub-images with increasing detail.
Nested spectral decoding compresses an image into a causal 1D sequence where each token corresponds to a disjoint set of frequencies and achieves token efficiency by reusing previous tokens.
To further enhance token efficiency, we introduce the non-uniform token-frequency mapping technique.
Since high-frequency components have low magnitude and minimal impact on the visual quality of images, we can encode them with coarser granularity compared to the low-frequency counterparts.
</p>

<p align="center">
    <img src="images/comparison.png" width="90%">
</p>


<h2>Spectral Autoregressive Generation</h2><hr>
<p>
we take the spectral tokens from the nested spectral tokenziation as the autoregressive targets.
Since the spectral tokens are trained in a nested manner to reconstruct sub-images of increasing levels of detail, each spectral token is expected to enhance the quality of the sub-image represented by previous tokens from the spectral domain.
This progressive refinement process aligns with human visual perception and artistic painting, both of which start with the overall structure and gradually focus on details.
This similarity qualitatively validates the rationale for performing causal autoregressive generation in the spectral domain.
We further provide some quantitative analysis through a toy experiment in the paper.
<p>

<p align="center">
  <img src="images/framework.png" width="90%">
</p>


<h2>Results</h2><hr>

<p>We perform three tasks: 3D semantic occupancy prediction, LiDAR segmentation, and semantic scene completion (SSC). For all tasks, our model only uses RGB images as inputs. </p>

<h4>3D Semantic Occupancy Prediction</h4><hr>

As dense semantic labels are difficult to obtain, we formulate a practical yet challenging task for vision-based 3D semantic occupancy prediction.
Under this task, the model is only trained using sparse semantic labels (LiDAR points) but is required to produce a semantic occupancy for all the voxels in the concerned 3D space during testing.
Our method is the first to demonstrate effective results on this challenging task.


<p></p>

<p align="center">
  <img src="images/vis1.png" width="90%">
</p>
<b>Visualization results on 3D semantic occupancy prediction and nuScenes LiDAR segmentation.</b> Our method can generate more comprehensive prediction results than the LiDAR segmentation ground truth.

<p> Given the simplicity of our segmentation head, we can adjust the resolution of TPV planes at test time arbitrarily without retraining the network.  </p>

<p></p>
<p align="center">
  <img src="images/vis2.png" width="90%">
</p>
<b>Arbitrary resolution at test time.</b> We can adjust the prediction resolution through interpolation at test time.
As resolution increases, more details about the 3D objects are captured.

<p></p>
<p align="center">
  <img src="images/vis3.png" width="90%">
</p>
<b>Prediction of small and rare objects.</b> We highlight bicycles, motorcycles and pedestrians with red, blue and yellow circles, respectively.
Note that although some of these objects are barely visible in RGB images, our model still predicts them successfully.

<p></p>
<h4>LiDAR Segmentation</h4><hr>

<p align="center">
  <img src="images/lidar_seg.png" width="90%">
</p>
<p><b>LiDAR segmentation results on nuScenes test set.</b> 
Despite critical modal difference, our TPVFormer-Base achieves comparable performance with LiDAR-based methods.
This is nontrivial since our method needs to reconstruct the complete 3D scene at a high resolution from only 2D image input, while the 3D structural information is readily available in the point clouds for LiDAR-based methods.</p>

<p align="center">
  <img src="images/nuscenes.png" width="90%">
</p>
<p><b>Official camera-only LiDAR segmentation leaderboard on nuScenes.</b> 
We are the first to demonstrate the potential of vision-based methods on LiDAR segmentation.


<h4>Semantic Scene Completion</h4>

<p align="center">
  <img src="images/ssc.png" width="90%">
</p>
<p><b>Semantic scene completion results on SemanticKITTI test set.</b> 
  TPVFormer outperforms all other methods in both IoU and mIoU, which demonstrates the effectiveness of TPVFormer in occupancy and semantics prediction.</p>



<p>
<div class="card">
<h3 class="card-header">Bibtex</h3>
<div class="card-block">
<pre class="card-text clickselect">
@article{huang2023tri,
    title={Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction},
    author={Huang, Yuanhui and Zheng, Wenzhao and Zhang, Yunpeng and Zhou, Jie and Lu, Jiwen},
    journal={arXiv preprint arXiv:2302.07817},
    year={2023}
}
</pre>
</div>
</div>
</p>


<p align="right">
     <a href="https://hanlab.mit.edu/projects/anycost-gan/">Website Template</a>
</p>

</div>
</div> <!-- row -->

</div> <!-- container -->

<script>
  var coll = document.getElementsByClassName("collapsible");
  var i;
  
  for (i = 0; i < coll.length; i++) {
    coll[i].addEventListener("click", function() {
      this.classList.toggle("active");
      var content = this.nextElementSibling;
      if (content.style.maxHeight){
        content.style.maxHeight = null;
      } else {
        content.style.maxHeight = content.scrollHeight * 50+ "px";
      } 
      content.style.height = "550%";
    });
  }
</script>

</body>
</html>